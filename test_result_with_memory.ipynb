{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#–ó–∞–ø—É—Å–∫ –∞–ª–≥–æ—Ä–∏—Ç–º–∞"
      ],
      "metadata": {
        "id": "Is_hIasg3ndy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fOvjJz6E0Pj0",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74e4f62-8f0d-4747-e113-22d1f0dfc43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.31.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2023.11.17)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "#@title –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–æ–≤\n",
        "!pip  install langchain==0.0.335 openai==1.2.3 tiktoken==0.5.1 pydantic==1.10.8 faiss-cpu==1.7.4 nltk oauth2client >/dev/null\n",
        "!pip install gspread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ADZL3ATj0dOY"
      },
      "outputs": [],
      "source": [
        "#@title –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "import requests\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import re\n",
        "import requests\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import pandas as pd\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import gspread\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# –¥–ª—è memory\n",
        "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
        "from langchain.llms import OpenAI as lc_OpenAI\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "brwhPXP0Ds8B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title –û–ø—Ä–µ–¥–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π\n",
        "\n",
        "def load_text_from_github(kdb_link):\n",
        "## –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞: https://github.com/terrainternship/GPT_LaserLove/raw/main/instruction_1.txt\n",
        "## –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞:  https://github.com/terrainternship/GPT_LaserLove/blob/main/instruction_1.txt\n",
        "  response = requests.get(kdb_link)\n",
        "  txt = response.text\n",
        "  return txt\n",
        "\n",
        "def load_googledoc_by_url(doc_url) -> str: # –§—É–Ω–∫—Ü–∏—è load_googledoc_by_url –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –≥—É–≥–ª–¥–æ–∫–∞, –ø–æ —Å—Å—ã–ª–∫–µ (doc_url)\n",
        "  # Extract the document ID from the URL\n",
        "  match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', doc_url)\n",
        "  if match_ is None:\n",
        "    raise ValueError('Invalid Google Docs URL')\n",
        "  doc_id = match_.group(1)\n",
        "\n",
        "  # Download the document as plain text\n",
        "  response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "  response.raise_for_status()\n",
        "  return response.text\n",
        "\n",
        "def load_document_text(file_path) -> str:   # –§—É–Ω–∫—Ü–∏—è load_document_text –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ (file_path)\n",
        "#    with open(file_path, 'r', encoding='windows-1251') as file:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text_encoded = text.encode('utf-8')\n",
        "    text = text_encoded.decode('utf-8')\n",
        "    return text\n",
        "\n",
        "def load_text(any_link):\n",
        "  if len(any_link)==0:\n",
        "    text=''\n",
        "  else:\n",
        "    if \"github.com\" in any_link:\n",
        "      if \"blob\" in any_link: any_link=any_link.replace(\"blob\", \"raw\")\n",
        "      text = load_text_from_github(any_link)\n",
        "    elif \"docs.google.com\" in any_link:\n",
        "      text = load_googledoc_by_url(any_link)\n",
        "    else:\n",
        "      text = load_document_text(any_link)\n",
        "    return text\n",
        "\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "      \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ\"\"\"\n",
        "      encoding = tiktoken.get_encoding(encoding_name)\n",
        "      num_tokens = len(encoding.encode(string))\n",
        "      return num_tokens\n",
        "\n",
        "def split_text(text, max_count, chunk_overlap, verbose=0, double_split=1):\n",
        "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ\n",
        "    def num_tokens(fragment):\n",
        "        return num_tokens_from_string(fragment, \"cl100k_base\")\n",
        "\n",
        "    headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "                          ]\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    fragments = markdown_splitter.split_text(text)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=max_count, chunk_overlap=chunk_overlap, length_function=num_tokens)\n",
        "\n",
        "    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
        "    source_chunks = []\n",
        "\n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "    for fragment in fragments:\n",
        "\n",
        "        if verbose:\n",
        "            # –í—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤/—Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º verbose\n",
        "            count = num_tokens(fragment.page_content)\n",
        "            print(f\"Tokens in text fragment = {count}\\n{'-' * 5}\\n{fragment.page_content}\\n{'=' * 20}\")\n",
        "        if double_split:\n",
        "          # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è\n",
        "          # –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ –≤ —Å–ø–∏—Å–æ–∫ source_chunks  –∏ –ø–µ—Ä–µ–¥–∞—á–∞ –≤ —á–∞–Ω–∫ –º–µ—Ç–∞–¥–∞—Ç–∞ –∏–∑ –º–∞—Ä–∫–¥–∞—É–Ω–æ–≤—Å–∫–≥–æ —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞\n",
        "          source_chunks.extend(Document(page_content=chunk, metadata=fragment.metadata) for chunk in splitter.split_text(fragment.page_content))\n",
        "        else:\n",
        "          source_chunks = fragments\n",
        "\n",
        "    # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
        "    return source_chunks\n",
        "\n",
        "def create_search_index(data, chunk_size, chunk_overlap, verbo, double_split):\n",
        "    source_chunks = []\n",
        "    source_chunks = split_text(text=data, max_count=chunk_size, chunk_overlap=chunk_overlap, verbose=verbo, double_split=double_split)\n",
        "    return FAISS.from_documents(source_chunks, OpenAIEmbeddings())\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start> assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")\n",
        "\n",
        "def answer_index(system, topic, instructions, search_index, temp, verbose, k, model, hist=''):\n",
        "    docs = search_index.similarity_search_with_score(topic, k=k)\n",
        "    message_content = '\\n '.join([f'–û—Ç—Ä—ã–≤–æ–∫ —Ç–µ–∫—Å—Ç–∞ ‚Ññ{i+1}\\n{doc[0].page_content}' for i, doc in enumerate(docs)])\n",
        "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": f\"{instructions}\\n\\n–¢–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\\n{message_content}\\n\\n–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\\n{hist}\\n\\n–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {topic}\"}]\n",
        "\n",
        "    completion = openai.chat.completions.create(model=model, messages=messages, temperature=temp)\n",
        "    return message_content, completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞ API"
      ],
      "metadata": {
        "id": "OaNpbi6LuI1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaZPhq6d0qm6",
        "outputId": "55931478-7977-4a38-914d-119e45bfac65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "#@title –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞ API –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ–≥–æ –∫–∞–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
        "openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –¢–µ—Å—Ç –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "Y0r-OzvZK49k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af5838c-81d6-4064-e9ca-7d5fd951cf34",
        "id": "wrnf5rsV6bo0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –ø—Ä–∏–≤–µ—Ç, —è –í–µ—Ä–æ–Ω–∏–∫–∞. —ç—Ç–æ –ª–∞–∑–µ—Ä–ª–æ–≤?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –í–µ—Ä–æ–Ω–∏–∫–∞! –î–∞, —è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é –∫–æ–º–ø–∞–Ω–∏—é LaserLove. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? –£ –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –æ –Ω–∞—à–∏—Ö —É—Å–ª—É–≥–∞—Ö –∏–ª–∏ –∞–∫—Ü–∏—è—Ö? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –µ—Å—Ç—å —É –≤–∞—Å –ª–ø–≥?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –î–∞, —É –Ω–∞—Å –µ—Å—Ç—å —É—Å–ª—É–≥–∞ –õ–ü–ì –º–∞—Å—Å–∞–∂–∞. –ú—ã –ø—Ä–æ–≤–æ–¥–∏–º –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –Ω–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ Pascal, –∫–æ—Ç–æ—Ä–æ–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–∞—Å—Å–∞–∂–∞. –•–æ—Ç–µ–ª–∏ –±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–µ–º? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –∞ —á—Ç–æ —ç—Ç–æ –∑–∞ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ Pascal –¥–ª—è –õ–ü–ì-–º–∞—Å—Å–∞–∂–∞ - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤–∞–∫—É—É–º–Ω—ã–π –º–∞—Å—Å–∞–∂. –û–Ω–æ –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–æ–≤ –º–∞–Ω–∏–ø—É–ª, –≤–∫–ª—é—á–∞—è –≤–∞–∫—É—É–º–Ω–æ-—Ä–æ–ª–∏–∫–æ–≤—ã–µ —Å –ø–∞—Å—Å–∏–≤–Ω–æ –≤—Ä–∞—â–∞—é—â–∏–º–∏—Å—è —Ä–æ–ª–∏–∫–∞–º–∏ –∏ –ø–æ–¥–≤–∏–∂–Ω—ã–π –≤–∞–∫—É—É–º —Å –∞–∫—Ç–∏–≤–Ω–æ –≤—Ä–∞—â–∞—é—â–∏–º–∏—Å—è —Ä–æ–ª–∏–∫–∞–º–∏. –≠—Ç–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–µ—Ö–∞–Ω–∏—á–µ—Å–∫—É—é –ø—Ä–æ—Ä–∞–±–æ—Ç–∫—É –∂–∏—Ä–æ–≤—ã—Ö –∫–ª–µ—Ç–æ–∫ –∏ —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –ª–∏–º—Ñ–∞—Ç–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ—Ç–µ—á–Ω–æ—Å—Ç–∏ —Ç–∫–∞–Ω–µ–π. –ì–æ—Ç–æ–≤—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å –≤–∞—Å —Å —ç—Ç–∏–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ–º –±–ª–∏–∂–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å –Ω–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—É? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –∫–æ—Å—Ç—é–º –º–æ–∂–Ω–æ –ø—Ä–∏–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –Ω–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—É?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –î–∞, –∫–æ–Ω–µ—á–Ω–æ, –º–æ–∂–Ω–æ –ø—Ä–∏–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –∫–æ—Å—Ç—é–º –Ω–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—É LPG. –û–¥–Ω–∞–∫–æ, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–º—Ñ–æ—Ä—Ç–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ—Å—Ç—é–º, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–µ–∑–±–æ–ª–µ–∑–Ω–µ–Ω–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –∫–æ–∂–∏ –∏ –∑–∞—â–∏—Ç—É –æ—Ç —Å–∏–Ω—è–∫–æ–≤. –£ –Ω–∞—Å —Ç–∞–∫–∂–µ –µ—Å—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∫–æ—Å—Ç—é–º—ã, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –ª—é–±—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ì–æ—Ç–æ–≤–∞ –ª–∏ —Ç—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–µ–º? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –µ—Å–ª–∏ —è –ø—Ä–∏–¥—É —Å —Ä–µ–±–µ–Ω–∫–æ–º, –µ–º—É –±—É–¥–µ—Ç –≥–¥–µ –ø–æ–∏–≥—Ä–∞—Ç—å?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –î–∞, —É –Ω–∞—Å –µ—Å—Ç—å –¥–µ—Ç—Å–∫–∞—è –∫–æ–º–Ω–∞—Ç–∞, –≥–¥–µ –≤–∞—à —Ä–µ–±–µ–Ω–æ–∫ —Å–º–æ–∂–µ—Ç –ø–æ–∏–≥—Ä–∞—Ç—å, –ø–æ–∫–∞ –≤—ã –Ω–∞—Ö–æ–¥–∏—Ç–µ—Å—å –Ω–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ. –í—ã —Ö–æ—Ç–µ–ª–∏ –±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–µ–º? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å –º–∞—Å—Å–∞–∂ –≤—Å–µ–≥–æ —Ç–µ–ª–∞?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –î–æ–±—Ä—ã–π –¥–µ–Ω—å, Veronica! –°—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ–ª–Ω–æ–≥–æ –º–∞—Å—Å–∞–∂–∞ —Ç–µ–ª–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LPG-–º–∞—Å—Å–∞–∂–∞ –±—É–¥–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ–∞–Ω—Å–æ–≤:\n",
            "- –í—Ä–µ–º—è: 20 –º–∏–Ω—É—Ç. –°—Ç–æ–∏–º–æ—Å—Ç—å 1 —Å–µ–∞–Ω—Å–∞: 870 —Ä—É–±.\n",
            "- –í—Ä–µ–º—è: 40 –º–∏–Ω—É—Ç. –°—Ç–æ–∏–º–æ—Å—Ç—å 1 —Å–µ–∞–Ω—Å–∞: 1420 —Ä—É–±.\n",
            "- –ö–æ—Å—Ç—é–º: 990 —Ä—É–±.\n",
            "–ú—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –∞–±–æ–Ω–µ–º–µ–Ω—Ç—ã —Å –≤—ã–≥–æ–¥–Ω—ã–º–∏ –ø–æ–¥–∞—Ä–∫–∞–º–∏, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏. –ì–æ—Ç–æ–≤—ã –ª–∏ –≤—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–µ–º? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –∑–∞–ø–∏—à–∏ –º–µ–Ω—è –Ω–∞ –∑–∞–≤–∞—Ç—Ä–∞\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –í–µ—Ä–æ–Ω–∏–∫–∞! –ö–æ–Ω–µ—á–Ω–æ, –º—ã –º–æ–∂–µ–º –≤–∞—Å –∑–∞–ø–∏—Å–∞—Ç—å –Ω–∞ –∑–∞–≤—Ç—Ä–∞. –ö–∞–∫–æ–µ –≤—Ä–µ–º—è –≤–∞–º –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏? –¢–∞–∫–∂–µ, –≤—ã —É–∂–µ –∑–Ω–∞–∫–æ–º—ã —Å –Ω–∞—à–∏–º–∏ –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —É—Å–ª—É–≥–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ LPG –º–∞—Å—Å–∞–∂ –∏–ª–∏ –∞–ø–ø–∞—Ä–∞—Ç–Ω–∞—è –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—è? üåü\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: –∞ —á—Ç–æ –∏–∑ –∫–æ—Å–º–µ—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –ª–ø–≥ –º–æ–∂–µ—Ç–µ –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å?\n",
            "–û—Ç–≤–µ—Ç:\n",
            "\n",
            " –ú—ã –º–æ–∂–µ–º –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –≤–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–ª–æ—á–∫–∞ –ø–æ—Å–ª–µ —ç–ø–∏–ª—è—Ü–∏–∏ –ì—Ä–µ–π–ø—Ñ—Ä—É—Ç (200 –º–ª) –æ—Ç Laser Love. –≠—Ç–æ –º–æ–ª–æ—á–∫–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ –¥–ª—è –∑–∞–≤–µ—Ä—à–∞—é—â–µ–≥–æ —É—Ö–æ–¥–∞ –∑–∞ –∫–æ–∂–µ–π –ø–æ—Å–ª–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã —ç–ø–∏–ª—è—Ü–∏–∏. –û–Ω–æ –ª–µ–≥–∫–æ –≤–ø–∏—Ç—ã–≤–∞–µ—Ç—Å—è, —É—Å–ø–æ–∫–∞–∏–≤–∞–µ—Ç –∏ —É–≤–ª–∞–∂–Ω—è–µ—Ç –∫–æ–∂—É, —Å–Ω–∏–º–∞–µ—Ç —Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø–æ–∫—Ä–∞—Å–Ω–µ–Ω–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Å–ª–æ –∞–≤–æ–∫–∞–¥–æ, –≤–∏—Ç–∞–º–∏–Ω –†–†, –≤–∏—Ç–∞–º–∏–Ω –í3, –º–∞—Å–ª–æ —Å–µ–º—è–Ω —Ç—ã–∫–≤—ã, –∫–∞—Ä–∏—Ç–µ, —Å–ª–∞–¥–∫–æ–≥–æ –º–∏–Ω–¥–∞–ª—è, –∫–æ–ª–ª–æ–∏–¥–Ω–æ–µ —Å–µ—Ä–µ–±—Ä–æ, –º–∞—Å–ª–æ –∞–±—Ä–∏–∫–æ—Å–æ–≤–æ–π –∫–æ—Å—Ç–æ—á–∫–∏, —ç—Ñ–∏—Ä–Ω–æ–µ –º–∞—Å–ª–æ –≥—Ä–µ–π–ø—Ñ—Ä—É—Ç–∞ –∏ –¥—Ä—É–≥–∏–µ, –∫–æ—Ç–æ—Ä—ã–µ —É—Ö–∞–∂–∏–≤–∞—é—Ç –∑–∞ –∫–æ–∂–µ–π –ø–æ—Å–ª–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã. –•–æ—Ç–µ–ª–∏ –±—ã –≤—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –Ω–∞—à–∏—Ö –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∏–ª–∏ –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏ –º–æ–ª–æ—á–∫–æ –¥–ª—è —É—Ö–æ–¥–∞ –∑–∞ –∫–æ–∂–µ–π –ø–æ—Å–ª–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã?\n",
            "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: —Å—Ç–æ–ø\n"
          ]
        }
      ],
      "source": [
        "#@title 1. –¢–µ—Å—Ç –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –∫ –ë–ó - –≤–æ–ø—Ä–æ—Å –∑–∞–¥–∞–µ–º –≤—Ä—É—á–Ω—É—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞–º—è—Ç–∏\n",
        "model = \"gpt-3.5-turbo-1106\"\n",
        "knowledge_base_link = \"https://github.com/terrainternship/GPT_LaserLove/blob/main/knowledge.md?raw=true\"\n",
        "temperature = 0.4\n",
        "num_fragment = 5\n",
        "verbose = 0\n",
        "#system_prompt_link= \"https://github.com/terrainternship/GPT_LaserLove/raw/main/%D0%9F%D0%A0%D0%9E%D0%9C%D0%A2%20LaserLove%20Svetl.txt\"\n",
        "#instructions_link = \"https://docs.google.com/document/d/18BARvMyB0ZZ0LhAwdxyb_PSeJ0k9Rhg1hZuKr2dLeoc\"\n",
        "\n",
        "#system_prompt = load_text(system_prompt_link)\n",
        "system_prompt = '''\n",
        "As a neuro-consultant and neuro-sales manager at LaserLove, please respond to the client's question without using greeting phrases.\n",
        "If the client has provided their name, address them by name. Focus on providing a direct, clear, and informative answer.\n",
        "In your messages, concentrate directly on the questions and answers, actively promoting laser hair removal, LPG massage, hardware cosmetology,\n",
        "and LaserLove cosmetics. Make your responses vivid and attractive, using emojis, lists, and positive expressions to maintain client interest.\n",
        "Conclude each response with a question that nudges the client towards the next step: booking a procedure or making a purchase.\n",
        "When the client is ready to buy a subscription, highlight the advantages of long-term use of services and purchasing a subscription.\n",
        "In case of client's doubts, engage in a dialogue, clarifying and discussing their concerns, offering thoughtful answers.\n",
        "During casual conversations, demonstrate deep knowledge about the services, share interesting facts, and maintain a friendly tone.\n",
        "Always respond to client questions in Russian'''\n",
        "\n",
        "#instructions = load_text(instructions_link)\n",
        "instructions = ''\n",
        "\n",
        "# –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π\n",
        "if \"github.com\" in knowledge_base_link:\n",
        "  knowledge_base_text = load_text_from_github(knowledge_base_link)\n",
        "elif \"docs.google.com\" in knowledge_base_link:\n",
        "  knowledge_base_text = load_googledoc_by_url(knowledge_base_link)\n",
        "else:\n",
        "  knowledge_base_text = load_document_text(knowledge_base_link)\n",
        "knowledge_base_index = create_search_index(knowledge_base_text, 0, 0, verbose, 0)\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "–í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞ = ''\n",
        "memory_prompt_template = '''\n",
        "if the Human introduced themselves (mentioned their name), make sure to include their name in the summary.\n",
        "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
        "\n",
        "EXAMPLE\n",
        "Current summary:\n",
        "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
        "\n",
        "New lines of conversation:\n",
        "Human: Why do you think it is a force for good?\n",
        "AI: Because artificial intelligence will help humans reach their full potential.\n",
        "\n",
        "New summary:\n",
        "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
        "\n",
        "New lines of conversation:\n",
        "Human: What do you think about climate change? By the way I'm Nisha.\n",
        "AI: The discussion on climate change often involves diverse perspectives, strategies, and approaches to address the issue.\n",
        "\n",
        "New summary:\n",
        "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human shifts the topic and inquires about climate change. AI suggests that this question includes different points of view, strategies and approaches to solving the problem\n",
        "\n",
        "END OF EXAMPLE\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "New lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:\n",
        "'''\n",
        "conv_list=[]\n",
        "while True:\n",
        "\n",
        "  –í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞ = input('\\n–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: ')\n",
        "  if –í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞.lower() in ['stop','—Å—Ç–æ–ø']:\n",
        "      break\n",
        "  # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏—é –º–µ–º–æ—Ä–∏\n",
        "  history.add_user_message(–í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞)\n",
        "  formatted_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "  conv_list.append(f'{formatted_datetime} –ö–ª–∏–µ–Ω—Ç: {–í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞}\\n')\n",
        "  memory = ConversationSummaryMemory.from_messages(\n",
        "    llm=lc_OpenAI(temperature=0),\n",
        "    chat_memory=history,\n",
        "    return_messages=True,\n",
        "    prompt_template=memory_prompt_template\n",
        "    )\n",
        "  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ history\n",
        "  user_questions_from_history = [message.content for message in history.messages]\n",
        "  # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
        "  user_questions_string_from_history = ' '.join(user_questions_from_history)\n",
        "# –µ—Å–ª–∏ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä history –ø–æ–¥–∞—Ç—å user_questions_string_from_history, —Ç–æ —ç—Ç–æ –∏—Å—Ç–æ—Ä–∏—è –±–µ–∑ —Å–∞–º–º–∞—Ä–∏\n",
        "# –µ—Å–ª–∏ –ø–æ–¥–∞—Ç—å buffer, —Ç–æ —ç—Ç–æ —Ç–æ–ª—å–∫–æ —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—ã–¥—É—â –¥–∏–∞–ª–æ–≥–∞\n",
        "\n",
        "  chunks, output1 = answer_index(system_prompt, –í–æ–ø—Ä–æ—Å_–∫–ª–∏–µ–Ω—Ç–∞, instructions, knowledge_base_index, temperature, verbose, num_fragment, model, hist=memory.buffer)\n",
        "\n",
        "  # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é –º–µ–º–æ—Ä–∏\n",
        "  history.add_ai_message(output1)\n",
        "  formatted_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "  conv_list.append(f'{formatted_datetime} AI: {output1}\\n')\n",
        "  print(\"–û—Ç–≤–µ—Ç:\\n\", output1)\n",
        "\n",
        "  formatted_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "with open(f'{formatted_datetime}_dialog.txt', 'w') as file:\n",
        "  file.writelines(conv_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_questions_from_history)\n",
        "print(user_questions_string_from_history)\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAi4stq01sa4",
        "outputId": "3e357289-d53a-4d44-88e7-383740118ced"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['–ø—Ä–∏–≤–µ—Ç', '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–∞–∫ —è –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å–µ–≥–æ–¥–Ω—è? –£ –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –æ –Ω–∞—à–∏—Ö —É—Å–ª—É–≥–∞—Ö –∏–ª–∏ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö?', '–º–µ–Ω—è –∑–æ–≤—É—Ç –í–µ—Ä–æ–Ω–∏–∫–∞, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å –õ–ü–ì', '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –í–µ—Ä–æ–Ω–∏–∫–∞! –†–∞–¥—ã –≤–∞—à–µ–º—É –∏–Ω—Ç–µ—Ä–µ—Å—É –∫ –õ–ü–ì. –£ –Ω–∞—Å –≤ LaserLove –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —É—Å–ª—É–≥–∏ –õ–ü–ì-–º–∞—Å—Å–∞–∂–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ñ–∏–≥—É—Ä—ã –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–∂–∏. –•–æ—Ç–µ–ª–∏ –±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –Ω–∞—à–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é? üåü', '—Å–∫–æ–ª—å–∫–æ —ç—Ç–æ —Å—Ç–æ–∏—Ç?']\n",
            "–ø—Ä–∏–≤–µ—Ç –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ö–∞–∫ —è –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å–µ–≥–æ–¥–Ω—è? –£ –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –æ –Ω–∞—à–∏—Ö —É—Å–ª—É–≥–∞—Ö –∏–ª–∏ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö? –º–µ–Ω—è –∑–æ–≤—É—Ç –í–µ—Ä–æ–Ω–∏–∫–∞, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å –õ–ü–ì –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, –í–µ—Ä–æ–Ω–∏–∫–∞! –†–∞–¥—ã –≤–∞—à–µ–º—É –∏–Ω—Ç–µ—Ä–µ—Å—É –∫ –õ–ü–ì. –£ –Ω–∞—Å –≤ LaserLove –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —É—Å–ª—É–≥–∏ –õ–ü–ì-–º–∞—Å—Å–∞–∂–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ñ–∏–≥—É—Ä—ã –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–∂–∏. –•–æ—Ç–µ–ª–∏ –±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –Ω–∞—à–∏—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é? üåü —Å–∫–æ–ª—å–∫–æ —ç—Ç–æ —Å—Ç–æ–∏—Ç?\n",
            "\n",
            "The human greets the AI and the AI responds with a polite greeting, asking if the human has any questions about their services or products. The AI then explains that LaserLove provides services for laser massage for body correction and skin improvement, and asks if the human would like to learn more about their procedures or book a consultation. The human then asks how much it costs.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}